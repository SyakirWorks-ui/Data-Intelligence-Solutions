{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678a1483",
   "metadata": {},
   "source": [
    "# Automated HR Analytics Data Pipeline\n",
    "**Project:** IBM HR Analytics - Attrition Analysis  \n",
    "**Process:** Data Engineering & Automated Pipeline  \n",
    "**Author:** [Your Name]\n",
    "\n",
    "---\n",
    "### Purpose\n",
    "This notebook automates the extraction, cleaning, and transformation of HR datasets. \n",
    "It is designed to handle batch processing of multiple raw files and ensure data integrity \n",
    "before exporting the results for Power BI visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ebd914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment initialized.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define directory paths\n",
    "RAW_DATA_DIR = '../data/raw/'\n",
    "PROCESSED_DATA_PATH = '../data/processed/HR_Attrition_Cleaned.csv'\n",
    "LOG_FILE_PATH = '../docs/pipeline_log.txt'\n",
    "\n",
    "print(\"‚úÖ Environment initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb066d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hr_data(df):\n",
    "    \"\"\"\n",
    "    Standardizes and cleans the input dataframe.\n",
    "    - Removes redundant features\n",
    "    - Maps categorical binaries to integers\n",
    "    - Performs age and distance binning\n",
    "    \"\"\"\n",
    "    # 1. Drop redundant columns with zero variance\n",
    "    # 'errors=ignore' ensures the script continues if columns were already removed\n",
    "    redundant_cols = ['Over18', 'EmployeeCount', 'StandardHours', 'EmployeeNumber']\n",
    "    df = df.drop(columns=redundant_cols, errors='ignore')\n",
    "\n",
    "    # 2. Map Categorical 'Yes/No' to Binary (1/0)\n",
    "    # This facilitates statistical calculation for Attrition Rate\n",
    "    binary_map = {'Yes': 1, 'No': 0}\n",
    "    \n",
    "    if 'Attrition' in df.columns:\n",
    "        df['Attrition'] = df['Attrition'].map(binary_map)\n",
    "    if 'OverTime' in df.columns:\n",
    "        df['OverTime'] = df['OverTime'].map(binary_map)\n",
    "\n",
    "    # 3. Feature Engineering: Age Binning\n",
    "    age_bins = [18, 25, 35, 45, 55, 100]\n",
    "    age_labels = ['18-25', '26-35', '36-45', '46-55', '55+']\n",
    "    df['AgeGroup'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "    # 4. Feature Engineering: Distance Binning\n",
    "    dist_bins = [0, 5, 15, 100]\n",
    "    dist_labels = ['Near', 'Far', 'Very Far']\n",
    "    df['DistanceGroup'] = pd.cut(df['DistanceFromHome'], bins=dist_bins, labels=dist_labels, right=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483ff8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully processed: WA_Fn-UseC_-HR-Employee-Attrition.csv\n",
      "\n",
      "üöÄ Pipeline Execution: 2026-01-02 09:49:48 | Total Rows: 1470\n"
     ]
    }
   ],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"\n",
    "    Orchestrates the data pipeline: Scans raw folder, validates, cleans, and merges data.\n",
    "    \"\"\"\n",
    "    # Scan for CSV files in the raw directory\n",
    "    if not os.path.exists(RAW_DATA_DIR):\n",
    "        print(f\"‚ùå Error: Directory '{RAW_DATA_DIR}' not found.\")\n",
    "        return\n",
    "\n",
    "    csv_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.csv')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"‚ÑπÔ∏è No new raw files found to process.\")\n",
    "        return\n",
    "\n",
    "    processed_list = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        full_path = os.path.join(RAW_DATA_DIR, file)\n",
    "        raw_df = pd.read_csv(full_path)\n",
    "        \n",
    "        # Integrity Check: Validate required columns\n",
    "        required_columns = ['Age', 'Attrition', 'DistanceFromHome']\n",
    "        if not all(col in raw_df.columns for col in required_columns):\n",
    "            print(f\"‚ö†Ô∏è Warning: Missing required columns in {file}. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # Execute cleaning function\n",
    "        cleaned_df = clean_hr_data(raw_df)\n",
    "        processed_list.append(cleaned_df)\n",
    "        print(f\"‚úÖ Successfully processed: {file}\")\n",
    "\n",
    "    # Merge and export if data exists\n",
    "    if processed_list:\n",
    "        final_dataset = pd.concat(processed_list, ignore_index=True)\n",
    "        final_dataset.to_csv(PROCESSED_DATA_PATH, index=False)\n",
    "        \n",
    "        # Logging\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        log_entry = f\"Pipeline Execution: {timestamp} | Total Rows: {len(final_dataset)}\"\n",
    "        print(f\"\\nüöÄ {log_entry}\")\n",
    "        \n",
    "        # Save log history\n",
    "        with open(LOG_FILE_PATH, 'a') as log_file:\n",
    "            log_file.write(log_entry + '\\n')\n",
    "    else:\n",
    "        print(\"‚ùå Pipeline failed: No valid data found.\")\n",
    "\n",
    "# Trigger the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
